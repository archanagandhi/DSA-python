# Big O Notation

Big O notation describes the complexity of your code using algebraic terms. It is the language we use for talking about how long an algorithm takes to run (time complexity) or how much memory is used by an algorithm (space complexity). Big-O notation can express the **best, worst, and average-case** running time of an algorithm

### 1. O(1) - Constant Time
O(1) means that it takes a constant time to run an algorithm, regardless of the size of the input. ( Best Case )
### 2. O(log n) - Logarithmic Time
The running time grows in proportion to the logarithm of the input size,the run time barely increases as you exponentially increase the input.
### 3. O(n) - Linear Time 
This means that the running time increases at most linearly with the size of the input
### 4. O(n²) - Quadratic Time
O(n²) means that the calculation runs in quadratic time, which is the squared size of the input data.

## Big O Complexity Chart
<img src="https://user-images.githubusercontent.com/74424757/120405280-d48c2680-c365-11eb-9273-31d93edb4201.PNG" width="500" height="350">


## [Cheatsheet](https://www.bigocheatsheet.com/)
![big-o-cheat-sheet-poster](https://user-images.githubusercontent.com/74424757/120403565-0dc29780-c362-11eb-89d4-137aef2bed65.png)

